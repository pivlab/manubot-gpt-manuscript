## Evaluations of AI-based revisions {#sec:results}

### Evaluation setup

<!-- In addition, there could be multiple different yet valid revisions with a varying degree of quality. -->
Assessing the performance of text generation tasks is challenging, and this is especially true for automatic revisions of scientific content.
In this context, we need to make sure the revision does not change the original meaning or introduce incorrect or misleading information.
For this reason, our approach emphasizes human assessments of the revisions to mitigate these issues, and we followed the same procedure in evaluating our tool.
We used three manuscripts of our own authorship (see below), which allowed us to more objectively assess changes in the original meaning and determine whether revisions retained important details.
During the prompt engineering phase (see below), we also used a unit testing framework to ensure that the revisions produced by our prompts met a minimum set of quality measures.

#### Language models

The paragraph is well-written and mostly free from errors.
However, there are a couple of minor points that could be clarified for better readability and consistency:

1.
The term "GPT-3 Davinci models" might be more clear if written as "Davinci versions of GPT-3" to emphasize that Davinci is a version or variation within the GPT-3 models.
2.
The phrase "the completion endpoint" could benefit from a brief explanation or context for readers who may not be familiar with API terminology.
3.
The word "latter" refers to the last of several mentioned items, but since there are three models discussed, it might be clearer to specify which model is being referred to by name instead of using "latter."
4.
Consistency in capitalization of "Davinci" should be maintained.
It appears as "Davinci" and "Davinci."

Here is the revised paragraph:

We evaluated our AI-assisted revision workflow using three GPT-3 models from OpenAI: `text-davinci-003`, `text-davinci-edit-001`, and `text-curie-001`.
The first two models are based on the most advanced Davinci versions of GPT-3 (see [OpenAI - GPT-3 models](https://platform.openai.com/docs/models/gpt-3)).
While `text-davinci-003` is a production-ready model designed for generating text completions, `text-davinci-edit-001` is tailored for the edits endpoint and was still in beta at the time of testing.
This model provides a more intuitive interface for revising manuscripts, as it requires two inputs: the instructions and the text to be revised.
The `text-curie-001` model is faster and more cost-effective than the Davinci models and is described as "very capable" by its creators (see [OpenAI - GPT-3 models](https://platform.openai.com/docs/models

#### Manuscripts

| Manuscript ID  | GitHub URL  | Title | Keywords |
|:-----|:-------|:--------------|:------|
| CCC | [greenelab/ccc-manuscript](https://github.com/greenelab/ccc-manuscript) | An efficient not-only-linear correlation coefficient based on machine learning | correlation coefficient, nonlinear relationships, gene expression |
| PhenoPLIER | [greenelab/phenoplier_manuscript](https://github.com/greenelab/phenoplier_manuscript) | Projecting genetic associations through gene expression patterns highlights disease etiology and drug mechanisms | genetic studies, functional genomics, gene co-expression, therapeutic targets, drug repurposing, clustering of complex traits |
| Manubot-AI | [greenelab/manubot-gpt-manuscript](https://github.com/greenelab/manubot-gpt-manuscript) | A publishing infrastructure for AI-assisted academic authoring | manubot, artificial intelligence, scholarly publishing, software |

Table: **Manuscripts used to evaluate the AI-based revision workflow.** The title and keywords of a manuscript are used in prompts for revising paragraphs. IDs are used in the text to refer to them. {#tbl:manuscripts}


<!-- Using these manuscripts, we tested and improved our prompts. -->
<!-- Our findings are reported below. -->
For the evaluation of our tool, we used three manuscripts of our own authorship (Table @tbl:manuscripts): the Clustermatch Correlation Coefficient (CCC) [@doi:10.1101/2022.06.15.496326], PhenoPLIER [@doi:10.1038/s41467-023-41057-4], and Manubot-AI (this manuscript).
The CCC is a new correlation coefficient evaluated with transcriptomic data, while PhenoPLIER is a framework that integrates three different methods applied in the field of genetic studies.
The CCC falls within the realm of computational biology, whereas PhenoPLIER pertains to genomic medicine.
The CCC describes one computational method applied to a single data type (correlation to gene expression).
In contrast, PhenoPLIER encompasses a framework that includes three distinct approaches (regression, clustering, and drug-disease prediction) utilizing data from genome-wide and transcriptome-wide association studies (GWAS and TWAS), gene expression, and transcriptional responses to small molecule perturbations.
Consequently, the CCC has a simpler structure, while PhenoPLIER is a more complex manuscript with additional figures and tables and a Methods section that contains equations.
The third manuscript, Manubot-AI, exemplifies a much simpler structure and was written and revised using our tool before submission, which provides a more realistic AI-based revision use case.

#### Evaluation using human assessments

<!-- We discuss below our findings based on these PRs across different sections of the manuscripts. -->
The paragraph is well-written and mostly clear, but there are a few minor adjustments that could be made for clarity and consistency:

1.
The use of parentheses is a bit inconsistent.
For example, "CCC, PhenoPLIER, and Manubot-AI" are listed without explanation, which might confuse the reader if these are not well-known terms or if their relevance is not clear from the context.
If these are titles of manuscripts, it might be clearer to state that explicitly.
2.
The phrase "used the three language models described above" assumes that the description of the models has been previously mentioned in the text.
If this is a standalone paragraph, it may be necessary to introduce the models or remove the reference to "above."
3.
The phrase "to produce one pull request (PR) per manuscript and model" could be slightly rephrased for better flow.
4.
The use of quotation marks and italics together for the PR titles is not wrong, but it's a bit unusual.
Typically, one or the other is used for emphasis or to denote titles.

Here's a revised version of the paragraph:

We activated the Manubot AI revision workflow in the GitHub repositories for three manuscripts: CCC, PhenoPLIER, and Manubot-AI.
This action added an "ai-revision" workflow to the "Actions" tab within each repository.
We manually initiated the workflow, employing three language models to generate a separate pull request (PR) for each combination of manuscript and model.
These PRs are available under the "Pull requests" tab in


When manually assessing the quality of the revisions, we considered whether the revision: 1) preserve the original meaning, 2) preserve important details, 4) introduced new and incorrect information, and 5) preserve the correct Markdown format (e.g., citations, equations).


#### Prompt engineering

The paragraph is well-structured and quite clear in detailing the testing procedures for the tool.
However, there are a few minor grammatical corrections and clarifications needed.
Here is the proofread version:

We extensively tested our tool, including prompts, using a unit testing framework.
Our unit tests cover the general processing of manuscript content (such as splitting by paragraphs), the generation of custom prompts using manuscript metadata, writing back text suggestions (ensuring that the original style is preserved as much as possible to minimize the number of changes), and, more importantly, some basic quality measures of the revised text.
The latter set of unit tests was used during our prompt engineering work, and they ensure that section-specific prompts yield revisions with a minimum set of quality measures.
For instance, we wrote unit tests to check that revised abstracts consist of a single paragraph, start with a capital letter, end with a period, and that no citations to other articles are included.
For the Introduction section, we check that a certain percentage of citations are kept, which also attempts to give the model some flexibility to remove text deemed unnecessary.
We found that adding the instruction "most of the citations to other academic papers are kept" to the prompt was enough to achieve this with the most capable model.
We also wrote unit tests to ensure the models returned citations in the correct Manubot/Markdown format (e.g., `[@doi:...]` or `[@arxiv:...]`), and found that no changes to the prompt were needed for this (i.e., the model automatically detected the correct format in most cases).
For the Results section, we included tests with short inline formulas in LaTeX (e.g., `$\gamma_l$`) and references to figures, tables, equations, or other sections (e.g., `Figure @id` or `Equation (@id)`), and found that, in the majority of cases, the most capable model was able to correctly keep them in the right format.
For the Methods section, in addition to the aforementioned tests, we also evaluated the ability of models to use the correct format for the definition of numbered, multiline equations, and found that the most capable model succeeded in most cases.
For this particular case, we needed to modify our prompt to explicitly mention the correct format for multiline equations (see prompt for Methods in Figure @fig:ai_revision).

Changes made:
1.
Changed "manuscript content" to "manuscript content" for consistency.
2.
Changed "were used" to "was used" to agree with the subject "set."
3.
Capitalized "Abstracts" when referring to the section title.
4.
Added commas for clarity in the list of checks for the revised abstracts.
5.
Changed "succedded" to "succeeded" to correct the spelling error.


We also included tests where the model is expected to fail in generating a revision (for instance, when the input paragraph is too long for the model's context length).
In these cases, we ensure that the tool returns a proper error message.
We ran our unit tests across all models under evaluation.


### General assessment of language models

The paragraph is well-structured and detailed, but there are a few areas that could be improved for clarity and grammar.
Here is a revised version:

Our initial human assessments of the three manuscripts and unit tests revealed that, although the Curie model was faster and less expensive, it failed to produce acceptable revisions for any of the manuscripts.
The pull requests (PRs) demonstrate that most of its suggestions were incoherent with the original text across all sections of the manuscripts.
The model clearly struggled to understand the revision instructions; in most cases, it did not generate meaningful revisions.
Instead, it either replaced the text with the instructions, inserted the manuscript's title at the beginning of the paragraph, consistently omitted citations to other articles (especially in the Introduction section), or added content that was not present in the original text.
Additionally, for similar reasons, we found that the quality of the revisions produced by the `text-davinci-edit-001` model (edits endpoint) was inferior to that of the `text-davinci-003` model (completion endpoint).
This discrepancy could be due to the edits endpoint still being in beta at the time of testing.
The `text-davinci-003` model delivered the best results for all manuscripts and across the various sections.
Consequently, we decided to concentrate on the `text-davinci-003` model for the remainder of the evaluation described below.

Changes made:
- Specified "pull requests (PRs)" to clarify the abbreviation.
- Corrected minor grammatical issues for better flow.
- Improved sentence structure for clarity.


### Revision of different sections

<!-- These are our subjective assessments of the quality of the revisions, and we encourage the reader to inspect the PRs for each manuscript and model to see the full diffs and make their own conclusions. -->
Following our criteria (see above), we inspected the PRs generated by the AI-based workflow and report on our assessment of the changes suggested by the tool across different sections of the manuscripts.
The reader can access the PRs in the manuscripts' GitHub repositories (Table @tbl:manuscripts) and also included as diff files in Supplementary File 1 (CCC), 2 (PhenoPLIER) and 3 (Manubot-AI).


The paragraph you provided is well-structured and mostly clear.
However, I've made a few minor adjustments for improved clarity and correctness:

Below, we present the differences between the original text and the revisions made by the tool in a `diff` format (obtained from GitHub).
Line numbers are included to illustrate the length differences.
Unless the AI suggestions represent a complete overhaul of the text, single words are underlined and highlighted in colors to more clearly show the differences within a single sentence.
Red indicates words removed by the tool, green indicates words added, and no underlining signifies words kept unchanged.
In GitHub repositories, the full diffs can be viewed by clicking on the "File changes" tab under each PR.


#### Abstract

![
**Abstract of CCC.**
Original text is on the left and suggested revision on the right.
Single words are not underlined/highlighed in this case because the revision completely overhauled the text.
](images/diffs/abstract/ccc-abstract.svg "Diffs - CCC abstract"){#fig:abstract:ccc width="75%"}

We applied the AI-based revision workflow to the CCC abstract (Figure @fig:abstract:ccc).
The tool completely rewrote the text, leaving only the last sentence mostly unchanged.
The revised text was significantly shorter and contained longer sentences than the original, which could potentially make the abstract slightly harder to read.
The revision omitted the first two sentences, which introduced correlation analyses and transcriptomics, and instead directly stated the purpose of the manuscript.
It also omitted details about the method (line 5), focusing on the aims and results obtained.
The conclusion, ending with the same last sentence, suggests a broader application of the coefficient to other data domains, aligning with the original intent of the CCC authors.
The main concepts remained intact in the revised text.


The revised text for the abstract of PhenoPLIER was significantly shortened (from 10 sentences in the original, to only 3 in the revised version).
However, in this case, important concepts (such as GWAS, TWAS, CRISPR) and a proper amount of background information were missing, producing a less informative abstract.


#### Introduction

![
**First paragraph in the Introduction section of CCC.**
Original text is on the left and suggested revision on the right.
](images/diffs/introduction/ccc-paragraph-01.svg "Diffs - CCC introduction paragraph 01"){#fig:intro:ccc width="75%"}

The tool significantly revised the Introduction section of the CCC (Figure @fig:intro:ccc), producing a more concise and clear introductory paragraph.
The revised first sentence concisely incorporated ideas from the original two sentences, introducing the concept of "large datasets" and the opportunities for scientific exploration they present.
The model generated a more concise second sentence that introduces the "need for efficient tools" to uncover "multiple relationships" within these datasets.
The third sentence connected nicely with the previous one.
All references to scientific literature were retained in the correct Manubot format, although our prompts did not specify the reference format.
The rest of the sentences in this section were also correctly revised and could be incorporated into the manuscript with minor or no further changes.


The paragraph you provided is mostly well-written, but I've made a few corrections and improvements:

We also observed a high-quality revision of the introduction to PhenoPLIER.
However, the model failed to maintain the format of citations in one paragraph.
Additionally, the model did not converge on a revised text for the last paragraph, and our tool left an error message as an HTML comment at the top: `The AI model returned an empty string`.
Debugging the prompts revealed this issue, which could be related to the complexity of the paragraph.
In these cases, rerunning the automated revision might resolve this type of issue.


#### Results

![
**A paragraph in the Results section of CCC.**
Original text is on the left and suggested revision on the right.
Single words are not underlined/highlighed in this case because the revision completely overhauled the text.
](images/diffs/results/ccc-paragraph-01.svg "Diffs - CCC results paragraph 01"){#fig:results:ccc width="75%"}

We tested the tool on a paragraph from the Results section of the CCC paper (Figure @fig:results:ccc).
This paragraph describes Figure 1 of the CCC manuscript [@doi:10.1101/2022.06.15.496326], which presents four different datasets, each with two variables, and identifies various relationships or patterns: random/independent, non-coexistence, quadratic, and two-lines.
The revised paragraph not only has fewer sentences that are slightly longer but also consistently uses the past tense, in contrast to the original, which contains tense shifts.
Additionally, the revised paragraph retains all citations.
Although not explicitly mentioned in the prompts for this section (as it is for introductions), including citations is important in this context.
Mathematical expressions were preserved in their original LaTeX format, and the figure was correctly referenced using Manubot syntax.
In the third sentence of the revised paragraph (line 3), the model generated an effective summary of how all coefficients performed in the last two nonlinear patterns and explained why CCC was able to capture them.
As human authors, we would make a single change to the end of this sentence to avoid repeating the word "complexity": "..., while CCC increased the model's ability to capture the relationships ~~by using different degrees of complexity~~." The revised paragraph is more concise and provides a clearer description of what the figure shows and the functionality of CCC.
We found it noteworthy that the model condensed some of the original concepts (lines 4 to 8) into three new sentences (lines 3 to 5) without losing meaning and with greater clarity and conciseness.
The model also produced high-quality revisions for several other paragraphs that would require only minor changes.


The paragraph you provided is already well-written, with only minor adjustments needed for clarity and flow.
Here is a slightly revised version:

Some paragraphs in CCC, however, required further revisions before they were ready to be incorporated into the manuscript.
For example, the model produced revised text for certain paragraphs that was shorter, more direct, and clearer.
However, it also omitted important details and occasionally altered the meaning of sentences.
To remedy this, we could retain the simplified sentence structure while reintegrating the missing details.


![
**A paragraph in the Results section of PhenoPLIER.**
Original text is on the left and suggested revision on the right.
Single words are not underlined/highlighed in this case because the revision completely overhauled the text.
](images/diffs/results/phenoplier-paragraph-01.svg "Diffs - PhenoPLIER results paragraph 01"){#fig:results:phenoplier width="75%"}


The paragraph is generally well-written and clear in its description of the model's performance and limitations.
However, there are a few minor adjustments that could improve its readability and correctness:

1.
The term "Manubot/Markdown format" might be clearer if written as "Manubot Markdown format" or "Manubot/Markdown" format, depending on whether "Manubot" and "Markdown" are meant to be seen as a combined format or as separate formats.

2.
The phrase "the model's output demonstrated the limitations" could be more specific.
It might be helpful to clarify that it is the output from the model's revision process that demonstrated the limitations.

3.
The use of parentheses is correct, but some of the content within them could be made more concise.

4.
The phrase "the new text describes an experiment that does not exist with a reference to a nonexisting section" might be clearer as "the new text describes a nonexistent experiment and refers to a nonexisting section."

5.
The last sentence could be split into two for better readability.

Here's a revised version of the paragraph:

When applied to the PhenoPLIER manuscript, the model produced high-quality revisions for most paragraphs, while preserving citations and references to figures, tables, and other sections within the Manubot Markdown format.
In some cases, important details were missing, but these could be easily added back, preserving the improved sentence structure of the revised version.
However, the model's revision output also highlighted the limitations of revising one paragraph at a time without considering the rest of the text.
For example, one paragraph described our CRISPR screening approach to assess whether top genes in a latent variable (LV) could represent good therapeutic targets.
The model generated a paragraph with a completely different meaning, as shown in Figure @fig:results:phenoplier.
It omitted the CRISPR screen and the gene symbols associated with lipid regulation, which were key elements in the original text.
Instead, it described a nonexistent experiment and referenced a nonexisting section.
This suggests that the model focused on the title and keywords of the manuscript, as listed in Table @tbl:manuscripts, which were part of every prompt (see Figure @fig:ai_revision).
For instance, it introduced the idea of "gene co-expression" analysis to identify "therapeutic targets" and replaced "sets of genes" with "clusters of genes," aligning more closely with the keyword "clustering." This was a poor model-based revision, indicating that the original paragraph may have been too short or disconnected from the rest.
It could be beneficial to merge it with the following one, which describes related follow-up experiments.


#### Discussion

In both the CCC and PhenoPLIER manuscripts, revisions to the discussion section appeared to be of high quality.
The model kept the correct format when necessary (e.g., using italics for gene symbols), maintained most of the citations, and improved the readability of the text in general.
Revisions for some paragraphs introduced minor mistakes that a human author could readily fix.

![
**A paragraph in the Discussion section of CCC.**
Original text is on the left and suggested revision on the right.
](images/diffs/discussion/ccc-paragraph-01.svg "Diffs - CCC discussion paragraph 01"){#fig:discussion:ccc width="75%"}

The paragraph is generally well-written, but there are a couple of small adjustments that could be made for clarity and grammar:

One paragraph of the CCC discusses how non-linear correlation coefficients could potentially impact genetic studies of complex traits (Figure @fig:discussion:ccc).
Although some minor changes could be made, we believe the revised text reads better than the original.
It is also interesting to note how the model understood the format of citations and constructed more complex structures from it.
For instance, the two articles referenced in lines 2 and 3 of the original text were correctly merged into a single citation block and separated with a ";" in line 2 of the revised text.

Changes include:
- Changed "not-only-linear" to "non-linear" for clarity.
- Added "to note" after "It is also interesting" for better readability.
- Changed "built" to "constructed" for a more appropriate word choice.
- Added "a" before ";" for grammatical correctness.


#### Methods

Prompts for the Methods section were the most challenging to design, especially when the sections included equations.
The prompt for Methods (Figure @fig:ai_revision) is more focused in keeping the technical details, which was especially important for PhenoPLIER, whose Methods section contains paragraphs with several mathematical expressions.

![
**A paragraph in the Methods section of PhenoPLIER.**
Original text is on the left and suggested revision on the right.
](images/diffs/methods/phenoplier-paragraph-01.svg "Diffs - PhenoPLIER methods paragraph 01"){#fig:methods:phenoplier width="75%"}

The paragraph is well-written and mostly clear, with only a few minor adjustments needed for clarity and consistency.
Here is the proofread version:

We revised a paragraph in PhenoPLIER that contained two numbered equations (Figure @fig:methods:phenoplier).
The model made very few changes, and all the equations, citations, and most of the original text were preserved.
However, we found it remarkable how the model identified an incorrect reference to a mathematical symbol (line 8) and corrected it in the revision (line 7).
Indeed, the equation with the univariate model used by PrediXcan (lines 4-6 in the original) includes the *true* effect size, $\gamma_l$ (`\gamma_l`), instead of the *estimated* one, $\hat{\gamma}_l$ (`\hat{\gamma}_l`).


In PhenoPLIER, we found one large paragraph with several equations that the model failed to revise, although it performed relatively well in revising the rest of the section.
In CCC, the revision of this section was good overall, with some minor and easy-to-fix issues as in the other sections.


The paragraph you provided is clear and well-structured, with no grammatical errors.
However, for improved clarity and flow, you might consider rephrasing it slightly.
Here is a revised version:

"We also encountered issues when revising one paragraph at a time without considering the broader context.
For example, in PhenoPLIER, one of the initial paragraphs references the linear models employed by S-PrediXcan and S-MultiXcan but fails to provide the corresponding equations or detailed explanations.
These details appear in subsequent paragraphs, yet because the model had not processed that information at the time, it prematurely attempted to insert those equations (formatted correctly in Manubot/Markdown)."


![
**A paragraph in the Methods section of ManubotAI.**
Original text is on the left and suggested revision on the right.
The revision (right) contains a repeated set of sentences at the top that we removed to improve the clarity of the figure.
](images/diffs/methods/manubotai-paragraph-01.svg "Diffs - ManubotAI methods paragraph 01"){#fig:methods:manubotai width="75%"}


When revising the Methods section of Manubot-AI (this manuscript), we encountered instances where the model inserted novel sentences containing incorrect information.
For example, in one paragraph, it introduced a formula (formatted correctly for Manubot) that purportedly predicts the cost of a revision run.
In another paragraph (Figure @fig:methods:manubotai), it added new sentences claiming that the model was "trained on a corpus of scientific papers from the same field as the manuscript" and that its suggested revisions yielded a "modified version of the manuscript that is ready for submission." While these represent important potential future developments, they do not accurately reflect the current state of the work.
