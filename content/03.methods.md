## Implementing AI-based revision into the Manubot publishing ecosystem

We propose a human-centric approach for the use of AI in manuscript writing, which consists of the following steps: 1) human authors write the manuscript content; 2) an LLM revises the manuscript, generating a set of suggested changes; 3) human authors review the suggested changes, and the approved edits are then integrated into the manuscript.
By focusing on human review, this approach attempts to mitigate the risk of generating incorrect or misleading information.
To implement this human-centric approach, we developed a tool called the Manubot AI Editor, which is part of the Manubot infrastructure for scholarly publishing [@doi:10.1371/journal.pcbi.1007128].

---

The paragraph is well-written and clear in its explanation of the proposed human-centric approach.
The steps are laid out logically, and the purpose of each step is clear.
The citation is formatted in a way that suggests it is meant for a specific citation style, likely for a scientific or academic paper.
However, without knowing the exact requirements of the citation style, it's difficult to assess its accuracy.
Assuming the citation is correctly formatted for the intended style, the paragraph does not need any corrections.

### Overview of the Manubot AI Editor

![
**AI-based revision applied on a Manubot-based manuscript.**
**a)** A manuscript (written with Manubot) with different sections.
**b)** The prompt generator integrates metadata using prompt templates to generate section-specific prompts for each paragraph.
If a paragraph belongs to a non-standard section, then a default prompt will be used to perform a basic revision only.
The prompt for the Methods section includes the formatting of equations with identifiers.
All sections' prompts include these instructions: *"the text grammar is correct, spelling errors are fixed, and the text has a clear sentence structure"*, although these are only shown for abstracts.
Our tool allows the user to provide a custom prompt instead of using the default ones shown here.
](images/figure_1.svg "AI-based revision applied on a Manubot manuscript"){#fig:ai_revision width="75%"}

The Manubot AI Editor is an AI-based revision infrastructure integrated into Manubot [@doi:10.1371/journal.pcbi.1007128], a tool designed for the collaborative writing of scientific manuscripts.
Manubot seamlessly integrates with popular version control platforms, such as GitHub, enabling authors to easily track changes and collaborate on writing projects in real-time.
Additionally, Manubot automates the generation of formatted manuscripts (e.g., HTML, PDF, DOCX; see Figure {@fig:ai_revision}a for the HTML output).
Built upon this modern and open paradigm, our Manubot AI Editor ([https://github.com/manubot/manubot-ai-editor](https://github.com/manubot/manubot-ai-editor)) comprises three components: 1) a Python library offering classes and functions to read the manuscript content and its metadata, invoke the LLM for automatic text revision, and write the results back; 2) a GitHub Actions workflow that utilizes our Python library within GitHub to maintain provenance information for transparency; 3) a prompt generator that incorporates the manuscript's metadata using prompt templates to create section-specific prompts for each paragraph (illustrated in Figure {@fig:ai_revision}b).


The GitHub Actions workflow enables users to effortlessly trigger an automated revision task on either the entire manuscript or specific sections thereof.
Upon activation, the manuscript is segmented by section and subsequently by paragraph (see Figure {@fig:ai_revision}b), which are then fed into the language model along with a series of custom prompts.
The model subsequently generates a revised version of the text.
Utilizing the GitHub API, our workflow initiates a new pull request, thereby allowing the user to review and adjust the output prior to merging the modifications into the manuscript.
This workflow distinguishes between contributions from the human user and those from the AI language model, a distinction that may become significant in light of potential future legal rulings that could reshape the copyright landscape concerning the outputs of generative models.


We utilized the [OpenAI API](https://openai.com/api/) for access to these models.
Since this API incurs a cost with each run that depends on the manuscript length, we implemented a workflow in GitHub Actions that can be manually triggered by the user.
Our implementation allows users to adjust the costs according to their needs by enabling them to select specific sections for revision instead of the entire manuscript.
Additionally, several model parameters can be adjusted to further fine-tune costs, such as the language model version (including the current GPT-3.5 Turbo and GPT-4, and potentially newly published ones), the level of risk the model will take, or the "quality" of the completions.
For instance, using the Davinci models, the cost per run is below $0.50 for most manuscripts.


### Implementation details

The paragraph is mostly clear, but it could be slightly restructured for improved clarity and flow.
Here's a revised version:

"To run the workflow, the user must specify the branch to be revised, select the files or sections of the manuscript (optional), choose the language model to use, and provide an optional custom prompt (with section-specific prompts used by default) along with the output branch name.
For more advanced users, it is also possible to modify most of the tool's behavior and the parameters of the language model."


When the workflow is triggered, it downloads the manuscript by cloning the specified branch.
It revises all the manuscript files or only some of them if the user specifies a subset.
Next, each paragraph in the file is read and submitted to the OpenAI API for revision.
If the request is successful, the tool will write the revised paragraph in place of the original one, using one sentence per line (which is the recommended format for the input text).
If the request fails, the tool might try again (up to five times by default) if it encounters a common error (such as "server overloaded") or a model-specific error that requires adjusting some of its parameters.
If the error cannot be handled or the maximum number of retries is reached, the original paragraph is written instead, with an HTML comment at the top explaining the cause of the error.
This allows the user to debug the problem and attempt to fix it if desired.


As shown in Figure {@fig:ai_revision}b, each API request comprises a prompt (the instructions given to the model) and the paragraph to be revised.
Unless the user specifies a custom prompt, the tool will use a section-specific prompt generator that incorporates the manuscript title and keywords.
Therefore, both must be accurate to obtain the best revision outcomes.
The other key component to process a paragraph is its section.
For instance, the abstract is a set of sentences with no citations, whereas a paragraph from the Introduction section includes several references to other scientific papers.
A paragraph in the Results section has fewer citations but many references to figures or tables and must provide enough details about the experiments to understand and interpret the outcomes.
The Methods section is more dependent on the type of paper but, in general, it has to provide technical details and sometimes mathematical formulas and equations.
Therefore, we designed section-specific prompts, which we found led to the most useful suggestions.
Figure and table captions, as well as paragraphs that contain only one or two sentences and fewer than sixty words, are not processed and are copied directly to the output file.


The section of a paragraph is automatically inferred from the file name using a simple strategy.
For instance, if "introduction" or "methods" is part of the file name, the tool will categorize the file accordingly.
If the tool fails to infer a section from the file name, the user has the option to manually specify the section to which the file belongs.
The section can be a standard one, such as abstract, introduction, results, methods, or discussion, for which a specific prompt is used (see Figure {@fig:ai_revision}b).
Alternatively, for a non-standard section, a default prompt is used to instruct the model to perform basic revision tasks.
These tasks include minimizing the use of jargon, ensuring the grammar is correct, fixing spelling errors, and making sure the text has a clear sentence structure.


### Properties of language models

The Manubot AI Editor uses the [Chat Completions API](https://platform.openai.com/docs/guides/text-generation/chat-completions-api) to process each paragraph.
We have tested our tool using the Davinci (`text-davinci-003`, based on the initial GPT-3 models) and GPT-3.5 Turbo models (`gpt-3.5-turbo`).
All models can be adjusted using different parameters (refer to [OpenAI - API Reference](https://platform.openai.com/docs/api-reference/chat/create)), and the most important ones can be easily adjusted using our tool.


Language models for text completion have a context length that indicates the limit of tokens they can process (tokens are common character sequences in text).
This limit includes the size of the prompt and the paragraph, as well as the maximum number of tokens to generate for the completion (parameter `max_tokens`).
To ensure we never exceed this context length, our AI-assisted revision software processes each paragraph of the manuscript with section-specific prompts, as shown in Figure {@fig:ai_revision}b.
This approach allows us to process large manuscripts by breaking them into smaller chunks of text.
However, since the language model only processes a single paragraph from a section, it can potentially lose the context needed to produce a better output.
Nonetheless, we find that the model still produces high-quality revisions (see [Results](#sec:results)).
Additionally, the maximum number of tokens (parameter `max_tokens`) is twice the estimated number of tokens in the paragraph (one token approximately represents four characters, see [OpenAI - Tokenizer](https://platform.openai.com/tokenizer)).
The tool automatically adjusts this parameter and performs the request again if a related error is returned by the API.
The user can also force the tool to either use a fixed value for `max_tokens` for all paragraphs or change the fraction of maximum tokens based on the estimated paragraph size (two by default).

Corrections made:
- Added a comma before "or change the fraction of maximum tokens" for clarity and to correctly separate items in a list.


The language models used are stochastic, meaning they generate a different revision for the same input paragraph each time.
This behavior can be adjusted by using the "sampling temperature" or "nucleus sampling" parameters (we use `temperature=0.5` by default).
Although we have selected default values that work well across multiple manuscripts, these parameters can be changed to make the model more deterministic.
The user can also instruct the model to generate several completions and select the one with the highest log probability per token, which can improve the quality of the revision.
Our implementation generates only one completion (parameter `best_of=1`) to avoid potentially high costs for the user.
Additionally, our workflow allows the user to process either the entire manuscript or individual sections.
This provides more cost-effective control while focusing on a single piece of text, wherein the user can run the tool several times and pick the preferred revised text.


### Installation and use

The Manubot AI Editor is part of the standard Manubot template manuscript, referred to as rootstock, and is available at [https://github.com/manubot/rootstock](https://github.com/manubot/rootstock).
Users wishing to utilize the workflow only need to follow the standard procedures to install Manubot.
The section titled "AI-assisted authoring," found in the `USAGE.md` file of the rootstock repository, explains how to enable the tool.
Afterward, the workflow (named `ai-revision`) will be available and ready to use under the Actions tab of the user's manuscript repository.
