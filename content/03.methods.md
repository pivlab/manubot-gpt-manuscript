## Implementing AI-based revision into the Manubot publishing ecosystem

The paragraph is well-structured and clear, with just a few minor adjustments needed for optimal flow and grammar:

We propose a human-centric approach to using AI in manuscript writing, which consists of the following steps: 1) human authors write the manuscript content; 2) an LLM revises the manuscript, generating a set of suggested changes; 3) human authors review the suggested changes, and the approved edits are then integrated into the manuscript.
By focusing on human review, this approach aims to mitigate the risk of generating incorrect or misleading information.
To implement this human-centric approach, we developed a tool called Manubot AI Editor, which integrates with the Manubot infrastructure for scholarly publishing (see [@doi:10.1371/journal.pcbi.1007128]).

Changes made:
- Changed "for the use of AI" to "to using AI" for better readability.
- Changed "attempts to mitigate" to "aims to mitigate" to convey a more definitive purpose.
- Removed "the" before "Manubot AI Editor" for proper naming.
- Added "see" before the citation to indicate that the reference provides additional information.

### Overview of the Manubot AI Editor

![
**AI-based revision applied on a Manubot-based manuscript.**
**a)** A manuscript (written with Manubot) with different sections.
**b)** The prompt generator integrates metadata using prompt templates to generate section-specific prompts for each paragraph.
If a paragraph belongs to a non-standard section, then a default prompt will be used to perform a basic revision only.
The prompt for the Methods section includes the formatting of equations with identifiers.
All sections' prompts include these instructions: *"the text grammar is correct, spelling errors are fixed, and the text has a clear sentence structure"*, although these are only shown for abstracts.
Our tool allows the user to provide a custom prompt instead of using the default ones shown here.
](images/figure_1.svg "AI-based revision applied on a Manubot manuscript"){#fig:ai_revision width="75%"}

The paragraph is well-written and mostly clear.
However, there are a few minor issues that could be addressed for improved readability and correctness:

1.
"AI-based revision infrastructure" could be rephrased as "AI-based editing infrastructure" for better clarity.
2.
The citation [@doi:10.1371/journal.pcbi.1007128] should be checked to ensure it is formatted correctly according to the style guide in use (e.g., APA, MLA, Chicago).
3.
"calls the LLM for automatic text revision" could be more specific by defining what "LLM" stands for on its first use.
4.
The use of "our" in "our Manubot AI Editor" and "our Python library" could be removed for a more formal and objective tone.
5.
The URL provided in the text should be checked to ensure it is correct and functional.
6.
The reference to figures (Figure {@fig:ai_revision}a and Figure {@fig:ai_revision}b) should be formatted correctly and ensure that the figures are properly labeled and referenced in the text.

Here's a revised version of the paragraph:

The Manubot AI Editor is an AI-based editing infrastructure incorporated into Manubot, a tool for the collaborative writing of scientific manuscripts [DOI: 10.1371/journal.pcbi.1007128].
Manubot integrates with popular version control platforms, such as GitHub, allowing authors to easily track changes and collaborate in real time.
Additionally, Manubot automates the generation of formatted manuscripts, including HTML, PDF, and DOCX outputs; for example, Figure 1a displays the HTML output.
Built on this modern, open paradigm, the Manubot AI Editor, available at [https://github.com/manubot/manubot-ai-editor](https://github.com/manubot/manubot-ai-editor), comprises three main components: 1) a Python library that offers classes and functions for reading manuscript content and metadata, invoking a Large Language Model (LLM) for automatic text revision, and writing back the results; 2) a GitHub Actions workflow that utilizes the Python library within GitHub to maintain provenance information for transparency; and 3) a prompt generator that leverages the manuscript's metadata with prompt templates to create section-specific prompts for each paragraph, as illustrated in Figure 1b.


The GitHub Actions workflow enables users to effortlessly initiate an automated revision task for either the entire manuscript or specific sections thereof.
Upon triggering the action, the manuscript is dissected by section and subsequently by paragraph (Figure {@fig:ai_revision}b), which are then submitted to the language model along with a series of custom prompts.
The model proceeds to deliver a revised version of the text.
Following this, our workflow utilizes the GitHub API to create a new pull request, thereby allowing users to examine and adjust the output prior to integrating the amendments into the manuscript.
This workflow distinguishes between text authored by the human user and that generated by the AI language model, a distinction that may gain significance in view of potential future legal rulings that could reshape the copyright terrain pertaining to the products of generative models.


The paragraph is well-written, but there are a few minor adjustments that could be made for clarity and consistency.
Here is a revised version:

We utilized the [OpenAI API](https://openai.com/api/) to access these models.
As this API incurs a cost for each run, which depends on the manuscript's length, we implemented a workflow in GitHub Actions that can be manually triggered by the user.
Our implementation enables users to tailor the costs to their needs by selecting specific sections of the manuscript for revision, rather than processing the entire document.
Additionally, users can adjust several model parameters to further fine-tune costs.
These include choosing the language model version (ranging from Davinci and Curie to the latest GPT-3.5 Turbo and GPT-4, as well as any newly released models), setting the level of risk the model will take, and determining the "quality" of the completions.
For example, using the Davinci models, the cost per run is typically less than $0.50 for most manuscripts.


### Implementation details

To run the workflow, the user must specify the branch that will be revised, select the files/sections of the manuscript (optional), specify the language model to use (`text-davinci-003` by default), an optional custom prompt (section-specific prompts are used by default), and provide the output branch name.
For more advanced users, it is also possible to change most of the tool's behavior or the language model parameters.


The paragraph is well-structured and mostly clear, but there are a few minor adjustments that could be made for clarity and correctness.
Here's a revised version:

When the workflow is triggered, it downloads the manuscript by cloning the specified branch.
It then reviews all of the manuscript files, or only a subset if specified by the user.
Next, each paragraph in the file is read and submitted to the OpenAI API for revision.
If the request is successful, the tool will overwrite the original paragraph with the revised one, formatting it with one sentence per line, which is the recommended format for input text.
If the request fails, the tool may retry up to five times by default if it encounters common errors, such as "server overloaded," or model-specific errors that require adjusting some parameters.
Should the error persist or the maximum number of retries be reached, the original paragraph is retained and an HTML comment is added above it to explain the cause of the error.
This enables the user to debug the problem and attempt a resolution if desired.

Changes made:
- Changed "revises" to "reviews" to clarify that the manuscript files are being looked over or checked.
- Added "then" after the first sentence for better flow.
- Changed "written" to "overwritten" for clarity on what happens to the original paragraph.
- Added "formatting it with" for clarity on how the revised paragraph is formatted.
- Added commas to break up the longer sentences for better readability.
- Changed "attempt to fix it" to "attempt a resolution" for conciseness and to avoid repetition of "fix".


The paragraph is well-written and mostly clear.
However, there are a few minor adjustments for clarity and grammar:

As shown in Figure {@fig:ai_revision}b, each API request consists of a prompt (the instructions given to the model) and the paragraph to be revised.
Unless the user specifies a custom prompt, the tool will use a section-specific prompt generator that incorporates the manuscript title and keywords; both must be accurate to obtain the best revision outcomes.
Another key component in processing a paragraph is its section.
For instance, the abstract is a set of sentences with no citations, whereas a paragraph from the Introduction section may have several references to other scientific papers.
A paragraph in the Results section typically has fewer citations but includes many references to figures or tables and must provide enough details about the experiments to enable understanding and interpretation of the outcomes.
The Methods section's content depends more on the type of paper, but generally, it must provide technical details and sometimes mathematical formulas and equations.
Therefore, we designed section-specific prompts, which we found led to the most useful suggestions.
Captions for figures and tables, as well as paragraphs that contain only one or two sentences and fewer than sixty words, are not processed and are copied directly to the output file.

Changes made:
1.
Changed "comprises" to "consists of" for clarity.
2.
Added a semicolon after "keywords" to better separate the ideas in the long sentence.
3.
Changed "has" to "may have" in the Introduction section sentence to acknowledge variability.
4.
Added "typically" to the Results section sentence to reflect common patterns without implying it's always the case.
5.
Added "the" before "Methods section's content" for grammatical correctness.
6.
Changed "less" to "fewer" before "than sixty words" to use the correct quantifier for countable nouns.


The paragraph is well-written and requires only a few minor adjustments for clarity and correctness.
Here is the revised version:

The section of a paragraph is automatically inferred from the file name using a simple strategy; for instance, the tool recognizes the section as "introduction" or "methods" if these terms are included in the file name.
If the tool fails to infer the section from the file name, the user can manually specify which section the file belongs to.
The section can be a standard one, such as abstract, introduction, results, methods, or discussion, for which a specific prompt is used (see Figure {@fig:ai_revision}b).
For non-standard sections, a default prompt is employed, instructing the model to perform basic revisions, such as "minimizing the use of jargon, ensuring the text's grammar is correct, fixing spelling errors, and making sure the text has clear sentence structure."


### Properties of language models

<!-- We mainly focused on the completion endpoint, as the edits endpoint was currently in beta. -->
The paragraph is well-written and mostly clear.
However, there are a few minor adjustments that could be made for clarity and consistency:

1.
"revision" might be more accurately described as "editing" or "review" when referring to the workflow process.
2.
"process each paragraph" could be more specific.
It's not clear whether the tool processes paragraphs of any text or specific types of text.
3.
The use of "our tool" at the end of the paragraph is a bit ambiguous.
It would be clearer if "the tool" were used throughout or if "our tool" were introduced earlier in the paragraph.
4.
The phrase "most important ones" is a bit vague.
It would be clearer to specify what "ones" refers to.

Here is a revised version of the paragraph:

Our AI-based editing workflow utilizes the [Chat Completions API](https://platform.openai.com/docs/guides/text-generation/chat-completions-api) to review and enhance text paragraphs.
We have conducted tests on our tool using both Davinci and Curie models from the GPT-3 series, which include `text-davinci-003`, `text-davinci-edit-001`, and `text-curie-001`.
Within the GPT-3 family, the Davinci models are the most advanced, offering the highest level of performance, while the Curie models are designed to be quicker and more cost-effective, albeit with less capability.
All models can be fine-tuned with various parameters (detailed in the [OpenAI - API Reference](https://platform


<!-- Therefore, it is not possible to use the entire manuscript as input, not even entire sections. -->
Language models for text completion have a context length that indicates the limit of tokens they can process (tokens are common character sequences in text).
This limit includes the size of the prompt and the generated text, as well as the maximum number of tokens to generate for the completion (parameter `max_tokens`).
For instance, the context length of DaVinci models is 4,000 and 2,048 for Curie (see [OpenAI - Models overview](https://platform.openai.com/docs/models/gpt-3)).
To ensure we never surpass this context length, our AI-assisted revision software processes each paragraph of the manuscript with section-specific prompts, as shown in Figure {@fig:ai_revision}b.
This approach allows us to process large manuscripts by breaking them into smaller chunks of text.
However, since the language model only processes a single paragraph from a section, it can potentially lose important context needed to produce a better output.
Nonetheless, we find that the model still produces high-quality revisions (see [Results](#sec:results)).
Additionally, the maximum number of tokens (parameter `max_tokens`) is set at twice the estimated number of tokens in the paragraph (one token approximately represents four characters, see [OpenAI - Tokenizer](https://platform.openai.com/tokenizer)).
The tool automatically adjusts this parameter and retries the request if a related error is returned by the API.
The user can also force the tool to either use a fixed value for `max_tokens` for all paragraphs or change the fraction of maximum tokens based on the estimated paragraph size (two by default).


The language models used are stochastic, meaning they generate a different revision for the same input paragraph each time.
This behavior can be adjusted by using the "sampling temperature" or "nucleus sampling" parameters (we use `temperature=0.5` by default).
Although we selected default values that work well across multiple manuscripts, these parameters can be changed to make the model more deterministic.
The user can also instruct the model to generate several completions and select the one with the highest log probability per token, which can improve the quality of the revision.
Our implementation generates only one completion (parameter `best_of=1`) to avoid potentially high costs for the user.
Additionally, our workflow allows the user to process either the entire manuscript or individual sections, providing more cost-effective control while focusing on a single piece of text.
In this way, the user can run the tool several times and pick the preferred revised text.


### Installation and use

The paragraph is well-written and mostly clear.
However, there are a few minor adjustments that could be made for clarity and consistency:

- "Manubot" should be consistently capitalized as it is a proper noun.
- The phrase "AI-assisted authoring" could be enclosed in quotation marks to indicate that it is a title or a specific section within the document.
- It would be clearer to specify that the "Actions tab" is on GitHub since not all readers may be familiar with GitHub's interface.

Here is the revised paragraph with these suggestions:

The Manubot AI Editor is part of the standard Manubot template manuscript, which is called Rootstock and available at [https://github.com/manubot/rootstock](https://github.com/manubot/rootstock).
Users who wish to use the workflow only need to follow the standard procedures to install Manubot.
The section "AI-assisted authoring" in the file `USAGE.md` of the Rootstock repository explains how to enable the tool.
After that, the workflow (named `ai-revision`) will be available and ready to use under the "Actions" tab of the user's manuscript repository on GitHub.
